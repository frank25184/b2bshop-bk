"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenAIService = void 0;
const openai_1 = __importDefault(require("openai"));
const dotenv_1 = __importDefault(require("dotenv"));
const logger_1 = require("../utils/logger");
// Load environment variables once at the application level
dotenv_1.default.config();
if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY not found in environment variables');
}
// Convert our ChatMessage to OpenAI's ChatCompletionMessageParam
function convertToOpenAIMessage(message) {
    // Basic conversion for common message types
    if (message.role === 'system' || message.role === 'user' || message.role === 'assistant') {
        return {
            role: message.role,
            content: message.content || '',
            ...(message.tool_calls ? { tool_calls: message.tool_calls } : {})
        };
    }
    // Handle tool messages
    if (message.role === 'tool') {
        return {
            role: 'tool',
            content: message.content || '',
            tool_call_id: message.tool_call_id || ''
        };
    }
    // Default fallback
    return {
        role: 'user',
        content: message.content || ''
    };
}
// Convert custom tool format to OpenAI's ChatCompletionTool format
function convertToOpenAITool(tool) {
    // If it's already in the correct format, return it
    if (tool.type === 'function' && typeof tool.type === 'string') {
        // Ensure the function name is valid
        if (!tool.function?.name || tool.function.name.trim() === '') {
            tool.function.name = `function_${Math.random().toString(36).substring(2, 9)}`;
        }
        return tool;
    }
    // Generate a valid function name if none is provided
    const functionName = tool.function?.name && tool.function.name.trim() !== ''
        ? tool.function.name
        : `function_${Math.random().toString(36).substring(2, 9)}`;
    // Otherwise, try to convert it
    return {
        type: 'function',
        function: {
            name: functionName,
            description: tool.function?.description || `Function ${functionName}`,
            parameters: tool.function?.parameters || {}
        }
    };
}
// Singleton instance for OpenAI client
let openAIInstance = null;
// Get cached OpenAI client instance
async function getOpenAIClient() {
    if (!openAIInstance) {
        openAIInstance = new openai_1.default({
            apiKey: process.env.OPENAI_API_KEY
        });
        await logger_1.Logger.debug('OpenAI client initialized');
    }
    return openAIInstance;
}
class OpenAIService {
    constructor(model = 'gpt-4o-mini') {
        this.client = null;
        this.model = model;
        logger_1.Logger.debug(`OpenAIService initialized with model: ${model}`);
    }
    // Lazy initialization of client
    async getClient() {
        if (!this.client) {
            this.client = await getOpenAIClient();
        }
        return this.client;
    }
    async generateText(prompt, systemPrompt = '', temperature = 0.7, tools, tool_choice) {
        await logger_1.Logger.startSpinner('Generating text with OpenAI...');
        const messages = [];
        if (systemPrompt) {
            messages.push({ role: 'system', content: systemPrompt });
        }
        messages.push({ role: 'user', content: prompt });
        try {
            // Convert messages to OpenAI format
            const openAIMessages = messages.map(convertToOpenAIMessage);
            // Convert tools to OpenAI format if provided
            const openAITools = tools ? tools.map(convertToOpenAITool) : undefined;
            const completion = await this.getClient().then(client => client.chat.completions.create({
                model: this.model,
                temperature,
                messages: openAIMessages,
                tools: openAITools,
                tool_choice
            }));
            const message = completion.choices[0]?.message;
            if (!message) {
                throw new Error('No response from OpenAI');
            }
            // Check for tool calls
            if (message.tool_calls && message.tool_calls.length > 0) {
                await logger_1.Logger.debug('Tool calls detected in generateText', { tool_calls: message.tool_calls });
                // For backward compatibility, we return a message about tool calls
                return 'The model wants to use tools. Please use generateChat or chatCompletion instead.';
            }
            const response = message.content;
            if (!response) {
                throw new Error('No content in response from OpenAI');
            }
            await logger_1.Logger.stopSpinner(true);
            await logger_1.Logger.section('Generated Response', response);
            return response;
        }
        catch (error) {
            await logger_1.Logger.stopSpinner(false);
            await logger_1.Logger.error('Error generating text', error);
            throw error;
        }
    }
    async generateChat(messages, temperature = 0.7, tools, tool_choice) {
        await logger_1.Logger.startSpinner('Generating chat response...');
        try {
            // Convert messages to OpenAI format
            const openAIMessages = messages.map(convertToOpenAIMessage);
            // Convert tools to OpenAI format if provided
            const openAITools = tools ? tools.map(convertToOpenAITool) : undefined;
            const completion = await this.getClient().then(client => client.chat.completions.create({
                model: this.model,
                temperature,
                messages: openAIMessages,
                tools: openAITools,
                tool_choice
            }));
            const response = completion.choices[0]?.message;
            if (!response) {
                throw new Error('No response from OpenAI');
            }
            await logger_1.Logger.stopSpinner(true);
            const result = {
                content: response.content || '',
                role: response.role
            };
            // Add tool calls if they exist
            if (response.tool_calls && response.tool_calls.length > 0) {
                result.tool_calls = response.tool_calls;
                await logger_1.Logger.debug('Tool calls detected', { tool_calls: result.tool_calls });
            }
            await logger_1.Logger.section('Chat Response', result.content);
            return result;
        }
        catch (error) {
            await logger_1.Logger.stopSpinner(false);
            await logger_1.Logger.error('Error generating chat response', error);
            throw error;
        }
    }
    async streamText(prompt, systemPrompt = '', temperature = 0.7, onToken, tools, tool_choice, onToolCall) {
        await logger_1.Logger.debug('Starting text stream...', {
            model: this.model,
            temperature
        });
        const messages = [];
        if (systemPrompt) {
            messages.push({ role: 'system', content: systemPrompt });
        }
        messages.push({ role: 'user', content: prompt });
        try {
            // Convert messages to OpenAI format
            const openAIMessages = messages.map(convertToOpenAIMessage);
            // Convert tools to OpenAI format if provided
            const openAITools = tools ? tools.map(convertToOpenAITool) : undefined;
            const stream = await this.getClient().then(client => client.chat.completions.create({
                model: this.model,
                temperature,
                messages: openAIMessages,
                stream: true,
                tools: openAITools,
                tool_choice
            }));
            let fullResponse = '';
            const toolCalls = {};
            for await (const chunk of stream) {
                const delta = chunk.choices[0]?.delta;
                // Handle content tokens
                if (delta?.content) {
                    const token = delta.content;
                    fullResponse += token;
                    onToken(token);
                }
                // Handle tool calls
                if (delta?.tool_calls && delta.tool_calls.length > 0) {
                    for (const toolCall of delta.tool_calls) {
                        const { index } = toolCall;
                        if (!toolCalls[index]) {
                            toolCalls[index] = {
                                id: toolCall.id,
                                type: toolCall.type,
                                function: {
                                    name: toolCall.function?.name || '',
                                    arguments: ''
                                }
                            };
                        }
                        // Accumulate function arguments
                        if (toolCall.function?.arguments) {
                            toolCalls[index].function.arguments += toolCall.function.arguments;
                        }
                        // Call the onToolCall callback if provided
                        if (onToolCall) {
                            onToolCall(toolCalls[index]);
                        }
                    }
                }
            }
            await logger_1.Logger.debug('Stream completed successfully');
        }
        catch (error) {
            await logger_1.Logger.error('Error in text stream', error);
            throw error;
        }
    }
    async chatCompletion(messages, temperature = 0.7, tools, tool_choice) {
        await logger_1.Logger.startSpinner('Chat completion with OpenAI...');
        try {
            // Convert messages to OpenAI format
            const openAIMessages = messages.map(convertToOpenAIMessage);
            // Convert tools to OpenAI format if provided
            const openAITools = tools ? tools.map(convertToOpenAITool) : undefined;
            const completion = await this.getClient().then(client => client.chat.completions.create({
                model: this.model,
                temperature,
                messages: openAIMessages,
                tools: openAITools,
                tool_choice
            }));
            // Safely access the message
            if (!completion.choices || completion.choices.length === 0 || !completion.choices[0].message) {
                throw new Error('No response from OpenAI');
            }
            const message = completion.choices[0].message;
            const response = {
                content: message.content || '',
                role: message.role
            };
            // Add tool calls if they exist
            if (message.tool_calls && message.tool_calls.length > 0) {
                response.tool_calls = message.tool_calls;
                await logger_1.Logger.debug('Tool calls detected', { tool_calls: response.tool_calls });
            }
            await logger_1.Logger.stopSpinner(true);
            await logger_1.Logger.section('Chat Completion Response', response.content);
            return response;
        }
        catch (error) {
            await logger_1.Logger.stopSpinner(false);
            await logger_1.Logger.error('Error in chat completion', error);
            throw error;
        }
    }
}
exports.OpenAIService = OpenAIService;
